---
title: "Chapter 1: Probability and inference"
output: github_document
---

```{r, echo = FALSE}
library(Cairo)
knitr::opts_chunk$set(
  fig.path = "ch01_figs/ch01-"
)
```

Libraries for models and helper functions for plots:

```{r message=FALSE, warning=FALSE}
library(brms)
library(coda)

col.alpha <- function( acol , alpha=0.2 ) {
    acol <- col2rgb(acol)
    acol <- rgb(acol[1]/255,acol[2]/255,acol[3]/255,alpha)
    acol
}

col.desat <- function( acol , amt=0.5 ) {
    acol <- col2rgb(acol)
    ahsv <- rgb2hsv(acol)
    ahsv[2] <- ahsv[2] * amt
    hsv( ahsv[1] , ahsv[2] , ahsv[3] )
}

rangi2 <- col.desat("blue", 0.5)
```

# 1.4 Discrete examples: genetics and spell checking

## 1.4.1 Inference about a genetic status

The first two sons don't have hemophilia. The likelihood of this is (0.5)(0.5) = 0.25 in the case that the mother has hemophilia, and (1)(1) = 1 in the case that the mother does not have hemophilia. We then use Bayes' rule to calculate the posterior probabilities, assuming a flat prior:

```{r}
prior <- c(1, 1)  # this could be (0.5, 0.5), the scale doesn't matter
likelihood <- c(0.25, 1)
posterior <- prior*likelihood
# standardize the posterior
( posterior <- posterior/sum(posterior) )
```

So there is a 20% chance that the mother has hemophilia and an 80% chance that she does not.

The likelihood that the third son doesn't have hemophilia is 0.5 in the case that the mother has it, and 1 if she doesn't. We use the posterior from the last calculation as our prior here:

```{r}
prior <- posterior
likelihood <- c(0.5, 1)
new_posterior <- prior*likelihood
# standardize the posterior
new_posterior/sum(new_posterior)
```

Now the mother has probability 0.111... (or 1/9) of having hemophilia.

If the third son does have hemophilia, the likelihood of this is 0.5 in the case that the mother has it too, but it's impossible (likelihood = 0) in the case that the mother doesn't have it. Then our new posterior is:

```{r}
prior <- posterior
likelihood <- c(0.5, 0)
new_posterior <- prior*likelihood
# standardize the posterior
new_posterior/sum(new_posterior)
```

This says that the mother must have hemophilia (probability 1), and that it is impossible that she doesn't (probability 0).

## 1.4.2 Spelling correction

```{r}
prior <- c(760, 60.5, 3.12)
likelihood <- c(0.00193, 0.000143, 0.975)
posterior <- prior*likelihood
posterior/sum(posterior)
```

Using this prior and likelihood, we conclude that there is a 67.3% chance that the writer intended to type "radom", a 32.5% chance that the writer intended to type "random", and an insignificant 0.2% chance that the writer intended to type "radon".

>When we dispute the claims of the posterior distribution, we are saying that the model does not fit the data or that we have additional prior information not included in the model so far.

# 1.6 Example: probabilities from football point spreads

## 1.6.1 Football point spreads and game outcomes

The data for this example is available from [Gelman's website](http://www.stat.columbia.edu/~gelman/book/data/).

```{r}
football <- read.table("football.asc", header = TRUE, skip = 7)[1:672,]
football$outcome <- football$favorite - football$underdog
```

## 1.6.2 Asssigning probabilities based on observed frequencies

Here we plot point spread versus outcome. The points are made slightly transparent so that overlapping points are easier to see.

```{r dev="CairoPNG"}
plot(outcome ~ spread, data = football, xlab = "point spread", pch = 16, col = col.alpha(rangi2, 0.4))
```

```{r}
# Pr(favorite wins) (ties are wins here)
sum(football$outcome >= 0 & football$spread >= 0)/nrow(football)
# Pr(favorite wins | spread = 3.5)
sum(football$outcome >= 0 & football$spread == 3.5)/sum(football$spread == 3.5)
# Pr(favorite wins by more than point spread)
sum(football$outcome > football$spread)/nrow(football)
# Pr(favorite wins by more than point spread | spread = 3.5)
sum(football$outcome > football$spread & football$spread == 3.5)/sum(football$spread == 3.5)
```

>These empirical probability assignments all seem sensible in that they match the intuition of knowledgeable football fans. However, such probability assignments are probelmatic for events with few directly relevant data points. For example, 8.5-point favorites won five out of five times during this three-year period, whereas 9-point favorites won thirteen out of twenty times. However, we realistically expect the probability of winning to be greater for a 9-point favorite than for an 8.5-point favorite. The small sample size with point spread 8.5 leads to imprecise probability assignments.

### Predicting the probability of a win with the spread using a binomial model

Let's take a look at the number of data points corresponding to each point spread.

```{r}
spreads <- unique(football[football$spread > 0,]$spread)
proportion_data <- data.frame(
    spread = spreads,
    games_played = sapply(spreads, function(s) sum(football$spread == s)),
    favorite_wins = sapply(spreads, function(s) sum(football$spread == s & football$outcome > 0))
)
proportion_data
```

There are many games with spreads less than 10, and fewer with higher spreads. Here is a plot showing the proportions of times the favorite team won corresponding to each spread, with the size of the data point corresponding to the number of games played with that spread.

```{r dev="CairoPNG"}
plot(
    0, 0,
    xlab = "spread", ylab = "proportion of games won by favorite",
    xlim = c(0.5*min(spreads), max(spreads)),
    ylim = c(0.38, 1.02),
    type = "n"
)

points(
    spreads, proportion_data$favorite_wins / proportion_data$games_played,
    pch = 21, bg = col.alpha(rangi2, 0.6), col = col.alpha("black", 0.6),
    cex = sqrt(proportion_data$games_played)
)
```

We want to model the relationship between the spread and the favorite team win rate, taking into account that there is more data available for some spreads than others. We want the larger points in the above plot to carry more weight in the model.

We'll fit a simple binomial model using a logit link with a linear dependence on spread.

```{r message=FALSE, warning=FALSE}
football_m1 <- brm(
    favorite_wins | trials(games_played) ~ spread,
    family = binomial(link = "logit"),
    data = proportion_data,
    cores = 4
)
```

```{r}
summary(football_m1)
```

```{r dev="CairoPNG"}
spread.seq <- seq(from = -1, to = 19, length.out = 30)

fitprops <- fitted(
    football_m1,
    newdata = list(
        games_played = rep(1, length(spread.seq)),
        spread = spread.seq
    ),
    probs = c(0.055, 0.945)
)

plot(
    0, 0,
    xlab = "spread", ylab = "proportion of games won by favorite",
    xlim = c(0.5, 18),
    ylim = c(0.38, 1.02),
    type = "n"
)

points(
    spreads, proportion_data$favorite_wins / proportion_data$games_played,
    pch = 21, bg = col.alpha(rangi2, 0.6), col = col.alpha("black", 0.6),
    cex = sqrt(proportion_data$games_played)
)

polygon(
    c(spread.seq, rev(spread.seq)), c(fitprops[,3], rev(fitprops[,4])),
    col = col.alpha("black", 0.15), border = NA)

lines(spread.seq, fitprops[,1])
```

This plot shows the mean fit curve in blue and its 89% credibility interval in gray. Note that the location of the curve is most heavily influenced by the larger dots.

## 1.6.3 A parametric model for the difference between outcome and point spread

```{r dev="CairoPNG"}
plot(
    outcome - spread ~ spread,
    data = football,
    xlab = "point spread", ylab = "outcome - point spread",
    pch = 16, col = col.alpha(rangi2, 0.4)
)

plot(density(football$outcome - football$spread, adj = 0.5), xlab = "outcome - point spread", main = NA)
curve(dnorm(x, 0, 14), col = rangi2, add = TRUE)
curve(
    dnorm(x, mean(football$outcome - football$spread), sd(football$outcome - football$spread)),
    col = col.desat("red", 0.5), add = TRUE
)
```

In the last plot, the wavy curve represents the empirical density of `outcome - spread`. The blue curve is the Normal(0, 14) density, and the red curve is the normal density with the same mean and standard deviation as `outcome - spread`. The two normal densities are extremely similar.

>...suggesting that the results of football games are approximately normally distributed with mean equal to the point spread and standard deviation nearly 14 points (two converted touchdowns).

>[In this model,] the probability that the favorite wins by more than the point spread is 1/2.

The new probabilities from this model:

```{r}
# Pr(favorite wins | spread = 3.5)
pnorm(0, mean = 3.5, sd = 14, lower.tail = FALSE)
# Pr(favorite wins | spread = 8.5)
pnorm(0, mean = 8.5, sd = 14, lower.tail = FALSE)
# Pr(favorite wins | spread = 9)
pnorm(0, mean = 9, sd = 14, lower.tail = FALSE)
```

### Estimating the relationship between outcome and point spread

```{r message=FALSE, warning=FALSE}
football_m2 <- brm(
    outcome ~ spread,
    data = football,
    iter = 6e3,  # more samples for a cleaner prediction interval below
    warmup = 1e3,
    chains = 4,
    cores = 4
)
```

```{r}
summary(football_m2)
```

As expected from the discussion just above, the coefficient for `spread` is very close to 1 and the intercept is very close to 0.

Below we plot the mean fit line and the 89% interval for this mean (the thinner shaded region). The wider and lighter shaded region is the 89% prediction interval, in which the model expects to see 89% of future data.

```{r dev="CairoPNG"}
spread.seq <- seq(from = -1, to = 19, length.out = 30)

fitoutcomes <- fitted(football_m2, newdata = list(spread = spread.seq), probs = c(0.055, 0.945))

prediction_interval <- apply(
    posterior_predict(football_m2, newdata = list(spread = spread.seq)),
    2, function(x) HPDinterval(as.mcmc(x), prob = 0.89)
)

plot(outcome ~ spread, data = football, xlab = "point spread", pch = 16, col = col.alpha(rangi2, 0.4))

polygon(
    c(spread.seq, rev(spread.seq)), c(fitoutcomes[,3], rev(fitoutcomes[,4])),
    col = col.alpha("black", 0.15), border = NA)

polygon(
    c(spread.seq, rev(spread.seq)), c(prediction_interval[1,], rev(prediction_interval[2,])),
    col = col.alpha("black", 0.1), border = NA
)

lines(spread.seq, fitoutcomes[,1])
```

Let's plot the new residuals:

```{r dev="CairoPNG"}
outcome_resid <- residuals(football_m2)[,1]
plot(density(outcome_resid, adj = 0.5), xlab = "outcome residuals", main = NA)
curve(dnorm(x, 0, sd(outcome_resid)), col = rangi2, add = TRUE)
```

This is virtually indistinguishable from the density for `outcome - spread` plotted above.

Here are the new probabilities we estimate:

```{r}
sigma <- summary(football_m2)$spec_pars["sigma", 1]
# Pr(favorite wins | spread = 3.5)
mu <- fitted(football_m2, newdata = list(spread = 3.5))[1,1]
pnorm(0, mean = mu, sd = sigma, lower.tail = FALSE)
# Pr(favorite wins | spread = 8.5)
mu <- fitted(football_m2, newdata = list(spread = 8.5))[1,1]
pnorm(0, mean = mu, sd = sigma, lower.tail = FALSE)
# Pr(favorite wins | spread = 9)
mu <- fitted(football_m2, newdata = list(spread = 9))[1,1]
pnorm(0, mean = mu, sd = sigma, lower.tail = FALSE)
```


### Comparison with the probabilities from the binomial model

Here are the probabilities from the binomial model:

```{r}
# Pr(favorite wins | spread = 3.5)
fitted(football_m1, newdata = list(games_played = 1, spread = 3.5), probs = c(0.055, 0.945))[1,]
# Pr(favorite wins | spread = 8.5)
fitted(football_m1, newdata = list(games_played = 1, spread = 8.5), probs = c(0.055, 0.945))[1,]
# Pr(favorite wins | spread = 9)
fitted(football_m1, newdata = list(games_played = 1, spread = 9), probs = c(0.055, 0.945))[1,]
```

***

[Antonio R. Vargas](https://github.com/szego)

03 Nov 2018
























































































